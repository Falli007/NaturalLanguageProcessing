{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution of AI Approaches\n",
        "\n",
        "## 1. Symbolic (1950s–1980s)\n",
        "- **Core idea:** Manually‐encoded rules and logic\n",
        "- **Example (Sentiment Analysis):**\n",
        "  ```text\n",
        "  IF \"happy\" in sentence → label = Positive  \n",
        "  IF \"sad\"   in sentence → label = Negative\n",
        "Pros:\n",
        "\n",
        "Transparent, easy to debug\n",
        "\n",
        "Leverages explicit domain knowledge\n",
        "\n",
        "Cons:\n",
        "\n",
        "Brittle: fails on unseen phrasing\n",
        "\n",
        "Requires extensive hand‐crafting of rules\n",
        "\n",
        "## 2. Statistical / “Classical” ML (1980s–2010)\n",
        "Core idea: Learn parameters from data (e.g. Naïve Bayes, Logistic Regression)\n",
        "\n",
        "Pros:\n",
        "\n",
        "Can generalize beyond fixed rules\n",
        "\n",
        "Requires less manual feature engineering than symbolic systems\n",
        "\n",
        "Cons:\n",
        "\n",
        "Limited model capacity → struggles with complex patterns\n",
        "\n",
        "Still needs domain‐specific feature design\n",
        "\n",
        "## 3. Neural / Deep Learning (2010–Present)\n",
        "Core idea: Multi‐layered neural networks learn features automatically\n",
        "\n",
        "Drivers:\n",
        "\n",
        "Compute power skyrocketed\n",
        "\n",
        "Data availability exploded\n",
        "\n",
        "Pros:\n",
        "\n",
        "Automatic feature learning from raw inputs\n",
        "\n",
        "State‐of‐the‐art performance on vision, speech, NLP, etc.\n",
        "\n",
        "Cons:\n",
        "\n",
        "Data‐hungry: performance scales with dataset size\n",
        "\n",
        "Compute‐hungry: requires GPUs/TPUs for training\n",
        "\n",
        "Opaque: “black‐box” models are harder to interpret\n",
        "\n",
        "## 4. Transformers & Attention Mechanisms\n",
        "Innovation: Self‐attention lets models weigh all tokens in a sequence\n",
        "\n",
        "Advantages over earlier NNs:\n",
        "\n",
        "Parallelizable: faster training than RNNs/LSTMs\n",
        "\n",
        "Long‐range context: captures dependencies across long texts\n",
        "\n",
        "Requirements:\n",
        "\n",
        "Even more data + compute (e.g., pretraining on billions of tokens)\n",
        "\n",
        "Why it matters:\n",
        "\n",
        "Foundation for models like BERT, GPT, T5, etc.\n",
        "\n",
        "Enables powerful transfer learning and few‐shot capabilities\n",
        "\n",
        "Key Takeaways\n",
        "Rule‐based → Statistical → Neural → Transformer\n",
        "\n",
        "Each wave trades off manual knowledge for data & compute\n",
        "\n",
        "Transformers represent the current frontier:\n",
        "\n",
        "Leverage huge corpora + attention to learn language structure\n",
        "\n",
        "Form the backbone of modern large‐language models (LLMs)#"
      ],
      "metadata": {
        "id": "C2cu96zZqSOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Vectors & Static Embeddings (2010s)\n",
        "\n",
        "### 1. Motivation\n",
        "- Represent each word as a **dense, low-dimensional** vector (50–300 dims) instead of a sparse one-hot (|V|-dimensional)  \n",
        "- Capture **semantic similarity**: words with similar contexts → nearby in vector space\n",
        "\n",
        "### 2. Mikolov’s **Word2Vec** (2013)\n",
        "- **Architectures**  \n",
        "  - **CBOW** (Continuous Bag-of-Words)  \n",
        "    - Predict target word from the sum/average of its context vectors  \n",
        "  - **Skip-Gram**  \n",
        "    - Predict surrounding context words given the target  \n",
        "- **Training objective**  \n",
        "  - Maximize probability of true context words, minimize for “negative‐sampled” noise words  \n",
        "- **Output**  \n",
        "  - A lookup table of word → ℝ^d embeddings, e.g.  \n",
        "    ```\n",
        "    “hello” → [0.043, 0.120, 0.722, …, 0.461]\n",
        "    “world” → [0.381, 0.520, 0.012, …, 0.201]\n",
        "    ```\n",
        "  - Typical d = 50–100 (or up to 300)\n",
        "\n",
        "### 3. Pros & Cons of Static Word Vectors\n",
        "\n",
        "| Pros                                            | Cons                                          |\n",
        "|-------------------------------------------------|-----------------------------------------------|\n",
        "|  Capture semantic & syntactic relationships  |  **Context-independent**: “bank” has one vector for finance & rivers |\n",
        "| Efficient to train & look up in downstream tasks |  Polysemy and homonymy not handled          |\n",
        "| Simple linear algebra analogies (e.g. **king – man + woman ≈ queen**) | OOV words & morphology issues              |\n",
        "\n",
        "### 4. Extensions & Alternatives\n",
        "- **GloVe** (Pennington et al.): leverages global word–word co-occurrence statistics  \n",
        "- **FastText** (Bojanowski et al.): enriches with subword (character n-gram) information  \n",
        "\n",
        "### 5. From Static to Contextual Embeddings\n",
        "1. **ELMo** (2018): context-sensitive embeddings from a bidirectional LSTM  \n",
        "2. **Transformer-based** (2018–):  \n",
        "   - BERT, GPT, T5, etc.  \n",
        "   - Produce **dynamic** embeddings per token **in its sentence**  \n",
        "   - Leverage attention to model long-range dependencies  \n",
        "\n",
        "---\n",
        "\n",
        "> **Key takeaway:**  \n",
        "> Static word vectors (Word2Vec) were a pivotal step—mapping words to ℝ^d to capture similarity—yet lacked context sensitivity. Transformers build on this by producing **contextual**, token‐level embeddings that adapt to each occurrence.  \n"
      ],
      "metadata": {
        "id": "0uGJ3v-qrn0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recurrent Neural Networks (RNNs)\n",
        "\n",
        "RNNs process sequences by maintaining a hidden state that “remembers” past inputs. At each time step \\(t\\):\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "h_t &= \\sigma\\big(W_x x_t + W_h h_{t-1} + b_h\\big) \\\\\n",
        "y_t &= \\phi\\big(V\\,h_t + b_y\\big) \\quad(\\text{optional})\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "For example, unrolling over “the quick brown fox jumps”:\n",
        "\n",
        "t=0: “the” → RNN → h₀\n",
        "t=1: “quick” → RNN → h₁\n",
        "t=2: “brown” → RNN → h₂\n",
        "…\n",
        "\n",
        "pgsql\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "### Vanishing & Exploding Gradients\n",
        "- **Backpropagation Through Time** multiplies gradients by \\(W_h\\) repeatedly.\n",
        "- If \\(\\|W_h\\|<1\\): gradients shrink → **vanish** → long-range dependencies lost.\n",
        "- If \\(\\|W_h\\|>1\\): gradients grow → **explode** → unstable training.\n",
        "\n",
        "### Pros & Cons\n",
        "\n",
        "| Pros                                        | Cons                                             |\n",
        "|---------------------------------------------|--------------------------------------------------|\n",
        "| ▶️ Captures sequential context via state    | ⚠️ Vanishing/exploding gradients over long spans |\n",
        "| ▶️ Shares parameters across time-steps      | ⚠️ Sequential computation → poor parallelism     |\n",
        "| ▶️ Simple for short sequences               | ⚠️ Limited memory for very long dependencies     |\n",
        "\n",
        "### Gated Variants & Evolution\n",
        "- **LSTM** (1997): adds input, forget & output gates to stabilize gradients  \n",
        "- **GRU** (2014): streamlined reset & update gates  \n",
        "- **Bidirectional RNNs**: process inputs both forwards & backwards  \n",
        "- **Transformers**: replace recurrence with self-attention for global context and full parallelism  "
      ],
      "metadata": {
        "id": "eWVSo5ECtZpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short-Term Memory (LSTM)\n",
        "\n",
        "LSTMs extend vanilla RNNs with a **cell state** \\(c_t\\) and three gating mechanisms to preserve long-range information and mitigate vanishing/exploding gradients.\n",
        "\n",
        "At each time step \\(t\\), given input \\(x_t\\) and previous hidden state \\(h_{t-1}\\) and cell state \\(c_{t-1}\\):\n",
        "\n",
        "```math\n",
        "f_t = \\sigma\\big(W_f[h_{t-1},\\,x_t] + b_f\\big)        &\\text{(forget gate)}\\\\\n",
        "i_t = \\sigma\\big(W_i[h_{t-1},\\,x_t] + b_i\\big)        &\\text{(input gate)}\\\\\n",
        "\\tilde c_t = \\tanh\\big(W_c[h_{t-1},\\,x_t] + b_c\\big)  &\\text{(cell candidate)}\\\\\n",
        "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde c_t        &\\text{(new cell state)}\\\\\n",
        "o_t = \\sigma\\big(W_o[h_{t-1},\\,x_t] + b_o\\big)        &\\text{(output gate)}\\\\\n",
        "h_t = o_t \\odot \\tanh(c_t)                            &\\text{(new hidden state)}\n",
        "When unrolled over “the quick brown fox jumps”:\n",
        "\n",
        "makefile\n",
        "Copy\n",
        "Edit\n",
        "t=0: “the”   → (h₀, c₀)\n",
        "t=1: “quick” → (h₁, c₁)\n",
        "t=2: “brown” → (h₂, c₂)\n",
        "…  \n",
        "t=4: “jumps” → (h₄, c₄)\n",
        "The cell state\n",
        "𝑐\n",
        "𝑡\n",
        "c\n",
        "t\n",
        "​\n",
        "  (highlighted in the diagram) flows along almost unchanged, letting the network carry information across many steps.\n",
        "\n",
        "Pros & Cons\n",
        "Pros\tCons\n",
        "▶️ Effectively captures long-range dependencies via gated cell state\t⚠️ More parameters & heavier compute\n",
        "▶️ Mitigates vanishing/exploding gradients\t⚠️ Inherently sequential → limited parallelism\n",
        "▶️ Robust selective memory through gates\t⚠️ More complex to tune & slower to train"
      ],
      "metadata": {
        "id": "SHdHbLwXt4TF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder–Decoder Attention\n",
        "\n",
        "When decoding at time $t$ (e.g. in a seq2seq LSTM), the decoder can “peek” at **all** encoder states via an attention module:\n",
        "\n",
        "1. **Inputs**  \n",
        "   - **Query** $Q$ = current decoder hidden state $h_t$  \n",
        "   - **Keys** $K$ = all encoder outputs $[h^{\\text{enc}}_1, \\dots, h^{\\text{enc}}_n]$  \n",
        "   - **Values** $V$ = same as $K$ (or a linear projection thereof)  \n",
        "\n",
        "2. **Scaled dot-product attention**  \n",
        "   $$\n",
        "   \\mathrm{Attention}(Q,K,V) \\;=\\;\\mathrm{softmax}\\!\\bigl(\\tfrac{QK^{T}}{\\sqrt{d_k}}\\bigr)\\,V\n",
        "   $$\n",
        "\n",
        "3. **Context vector** $c_t$  \n",
        "   - A weighted sum of encoder values:  \n",
        "     $$c_t = \\sum_{i=1}^n \\alpha_{t,i}\\,V_i,\\quad \\alpha_{t,i}=\\mathrm{softmax}_i\\bigl(QK^T/\\sqrt{d_k}\\bigr)$$  \n",
        "   - Concatenated with (or added to) the decoder state to produce the next output\n",
        "\n",
        "---\n",
        "\n",
        "## Self-Attention (“Attention Is All You Need”, 2017)\n",
        "\n",
        "Self-attention lets each token attend to **every** other token in the **same** sequence:\n",
        "\n",
        "1. **Within one sequence**, compute three projections per token:  \n",
        "   - **Queries** $Q=W_QX$  \n",
        "   - **Keys**    $K=W_KX$  \n",
        "   - **Values**  $V=W_VX$\n",
        "\n",
        "2. **Attention**  \n",
        "   $$\n",
        "   \\mathrm{SelfAttn}(X) = \\mathrm{softmax}\\!\\bigl(QK^T/\\sqrt{d_k}\\bigr)\\,V\n",
        "   $$\n",
        "\n",
        "3. **Multi-Head Attention**  \n",
        "   - Run self-attention $h$ times with different $(W_Q,W_K,W_V)$  \n",
        "   - Concatenate the $h$ outputs and project back to $d_{\\text{model}}$\n",
        "\n",
        "4. **Positional Encoding**  \n",
        "   - Since attention is permutation-invariant, add fixed or learned positional embeddings to $X$ so the model knows token order.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualization Example\n",
        "\n",
        "For the sentence  \n",
        "> “The animal didn’t cross the street because it was too tired.”\n",
        "\n",
        "- **Self-attention** at the token **“it”** will assign high weights to tokens like **“animal”** and **“street”**, letting the model resolve pronoun reference based on context.\n"
      ],
      "metadata": {
        "id": "bzwMuD-hvXSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head Attention\n",
        "\n",
        "Extends single “Scaled Dot-Product” attention by running it in parallel over multiple learned subspaces (“heads”) and then recombining:\n",
        "\n",
        "1. **Inputs**  \n",
        "   - Sequence of position-encoded embeddings \\(X \\in \\mathbb{R}^{n\\times d_\\text{model}}\\)  \n",
        "\n",
        "2. **Linear projections** (for each head \\(i=1,\\dots,h\\))  \n",
        "   \\[\n",
        "     Q_i = XW^Q_i,\\quad\n",
        "     K_i = XW^K_i,\\quad\n",
        "     V_i = XW^V_i\n",
        "   \\]\n",
        "   where \\(W^Q_i,W^K_i,W^V_i\\in\\mathbb{R}^{d_\\text{model}\\times d_k}\\)  \n",
        "\n",
        "3. **Per-head attention**  \n",
        "   \\[\n",
        "     \\mathrm{head}_i = \\mathrm{softmax}\\!\\Bigl(\\tfrac{Q_iK_i^\\top}{\\sqrt{d_k}}\\Bigr)\\,V_i\n",
        "   \\]\n",
        "\n",
        "4. **Concatenate & project**  \n",
        "   \\[\n",
        "     \\mathrm{MultiHead}(X)\n",
        "     = \\Concat(\\mathrm{head}_1,\\dots,\\mathrm{head}_h)\\,W^O,\\quad\n",
        "     W^O\\in\\mathbb{R}^{hd_k\\times d_\\text{model}}\n",
        "   \\]\n",
        "\n",
        "---\n",
        "\n",
        "## Positional Encoding\n",
        "\n",
        "Since attention is order-agnostic, add fixed sinusoidal signals to each token embedding so the model can infer position:\n",
        "\n",
        "For position \\(pos\\) and dimension index \\(i\\) (0-based):\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "\\mathrm{PE}_{(pos,2i)}   &= \\sin\\!\\Bigl(\\tfrac{pos}{10000^{2i/d_\\text{model}}}\\Bigr),\\\\\n",
        "\\mathrm{PE}_{(pos,2i+1)} &= \\cos\\!\\Bigl(\\tfrac{pos}{10000^{2i/d_\\text{model}}}\\Bigr).\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "- **Properties:**  \n",
        "  - Each dimension has a different frequency → unique positional “wave” patterns  \n",
        "  - Enables the model to learn to attend by relative and absolute positions  \n",
        "- **Usage:**  \n",
        "  \\(\\widetilde{X} = X + \\mathrm{PE}\\), then feed \\(\\widetilde{X}\\) into the Transformer layers.  \n"
      ],
      "metadata": {
        "id": "8JKKXsC93TNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer & BERT Output Heads\n",
        "\n",
        "## 1. Pre-training Heads\n",
        "\n",
        "- **Masked Language Modeling (MLM)**  \n",
        "  - **Input:** Transformer hidden states \\(H \\in \\mathbb{R}^{n\\times d}\\)  \n",
        "  - **Head:**  \n",
        "    1. Linear layer \\( \\mathbb{R}^d \\to \\mathbb{R}^{|V|} \\)  \n",
        "    2. Softmax over vocabulary  \n",
        "  - **Objective:** Predict masked tokens (e.g. “The [MASK] of France is Paris”)\n",
        "\n",
        "- **Next Sentence Prediction (NSP)**  \n",
        "  - **Input:** Final [CLS] hidden state \\(h_{\\text{[CLS]}}\\)  \n",
        "  - **Head:** Linear \\(\\to\\) softmax (2 classes: IsNext / NotNext)\n",
        "\n",
        "## 2. Fine-tuning Heads\n",
        "\n",
        "- **Question Answering (Span Extraction)**  \n",
        "  - **Inputs:** Sequence of hidden states \\(\\{h_1,\\dots,h_n\\}\\)  \n",
        "  - **Heads:**  \n",
        "    - **Start-position classifier:** Linear \\(\\to\\) softmax over \\(n\\) tokens  \n",
        "    - **End-position classifier:** Linear \\(\\to\\) softmax over \\(n\\) tokens  \n",
        "\n",
        "- **Sequence Classification**  \n",
        "  - **Input:** [CLS] hidden state \\(h_{\\text{[CLS]}}\\)  \n",
        "  - **Head:** Linear \\(\\to\\) softmax (or sigmoid) for \\(k\\) classes or binary\n",
        "\n",
        "## 3. Model Dimensions\n",
        "\n",
        "| Model       | Hidden Size \\(d\\) |  \n",
        "|-------------|-------------------|  \n",
        "| BERT-base   | 768               |  \n",
        "| BERT-large  | 1024              |  \n",
        "\n",
        "> **Note:** All these heads are lightweight (a single or pair of linear+softmax layers) appended to the shared Transformer representations.  \n"
      ],
      "metadata": {
        "id": "ueAd_AXC49Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for NLP"
      ],
      "metadata": {
        "id": "MXym2m0P5AjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword Removal\n",
        "\n",
        "Stopwords are high-frequency, low-information words (e.g. “the”, “is”, “and”) that are often filtered out before NLP tasks to reduce noise and dimensionality.\n",
        "\n",
        "### 1. Example Tweet\n",
        "> “I’m amazed how often in practice, not only does a @huggingface NLP model solve your problem, but one of their public finetuned checkpoints, is good enough for the job.  \n",
        "> Both impressed, and a little disappointed how rarely I get to actually train a model that matters :(”\n",
        "\n",
        "### 2. NLTK Stopword Filter\n",
        "\n",
        "```python\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 1. Load English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# 2. Tokenize & lowercase\n",
        "tokens = tweet.lower().split()\n",
        "\n",
        "# 3. Filter out stopwords\n",
        "filtered = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "print(\"Before:\", \" \".join(tokens))\n",
        "print(\"After: \", \" \".join(filtered))\n",
        "\n",
        "3. Before & After\n",
        "Before\tAfter\n",
        "i’m amazed how often in practice, not only does a @huggingface nlp model…\ti’m amazed often practice, @huggingface nlp model solve problem,…\n",
        "\n",
        "4. Pros & Cons\n",
        "Pros\tCons\n",
        "▶️ Reduces vocabulary size & computational cost\t⚠️ Might remove informative words (e.g. “not”)\n",
        "▶️ Simplifies downstream models\t⚠️ Static list → may not suit all domains\n",
        "▶️ Easy to implement with libraries (NLTK, spaCy)\t⚠️ Context-ignored filtering\n",
        "\n",
        "Key takeaway:\n",
        "Removing stopwords can speed up and simplify text processing, but choose your stopword list carefully (and consider task-specific tweaks) to avoid discarding crucial information."
      ],
      "metadata": {
        "id": "DkjBqvoWahGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization in NLP\n",
        "\n",
        "Tokenization is the process of breaking raw text into discrete units (“tokens”) that a model can ingest. Depending on your task and model, tokens can be:\n",
        "\n",
        "1. **Words**  \n",
        "   - e.g. “amazed”, “practice”  \n",
        "   - Intuitive, fast — but huge vocabularies & OOV (out-of-vocab) issues  \n",
        "\n",
        "2. **Subwords / WordPieces / BPE**  \n",
        "   - e.g. “amaz”, “##ed”, “practi”, “##ce”  \n",
        "   - Balances vocabulary size vs. ability to handle rare words/morphology  \n",
        "\n",
        "3. **Characters**  \n",
        "   - e.g. “I”, “’”, “m”, “ ”, “a”, “m”, “a”, “z”, “e”, “d”, …  \n",
        "   - Very small vocab; no OOV; longer sequences, slower models  \n",
        "\n",
        "4. **Punctuation & Symbols**  \n",
        "   - “,” “.” “!” “?” “@” “#” etc.  \n",
        "   - Often kept as separate tokens for sentiment/mention handling  \n",
        "\n",
        "5. **Special & Model-Specific Tokens**  \n",
        "   - **Normalization placeholders**:  \n",
        "     - `<URL>` for links  \n",
        "     - `<USER>` for Twitter handles (e.g. `@joebloggs`)  \n",
        "   - **Task tokens**:  \n",
        "     - `[CLS]`, `[SEP]`, `[MASK]` for BERT/Transformer inputs  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Preprocessing Steps\n",
        "\n",
        "Before tokenizing, you often:\n",
        "\n",
        "- **Lowercase** (optional): unify “The” vs. “the”  \n",
        "- **Normalize mentions/URLs**: map `@elonmusk` → `<USER>`, `http://…` → `<URL>`  \n",
        "- **Strip or isolate punctuation**: ensure “practice,” → “practice” + “,”  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Pythonic Examples\n",
        "\n",
        "```python\n",
        "tweet = \"@joebloggs thinks that the NLP models that @huggingface made are super cool https://t.co/abc123\"\n",
        "\n",
        "# Character tokens\n",
        "chars = [c for c in tweet]\n",
        "\n",
        "# Simple whitespace tokens\n",
        "words = tweet.split()\n",
        "\n",
        "# Replace mentions & URLs\n",
        "import re\n",
        "tweet_norm = re.sub(r'@\\w+', '<USER>', tweet)\n",
        "tweet_norm = re.sub(r'https?://\\S+', '<URL>', tweet_norm)\n",
        "\n",
        "# Subword (via HuggingFace tokenizer)\n",
        "from transformers import BertTokenizer\n",
        "tok = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "subwords = tok.tokenize(tweet_norm)\n",
        "\n",
        "3. Pros & Cons\n",
        "Token Type\tPros\tCons\n",
        "Words\tIntuitive, semantic\tLarge vocab, OOV\n",
        "Subwords (BPE)\tHandles rare words, moderate vocab\tSplits common words inconsistently\n",
        "Characters\tNo OOV, small vocab\tVery long sequences, slower to process\n",
        "Special Tokens\tNormalize noise (mentions, URLs), guide models\tRequires task-specific rules/regex\n",
        "\n",
        "Key takeaway:\n",
        "Choose your tokenization strategy based on your model and data: subwords are standard for transformers (BERT/GPT), whereas character or word-level may suit specialized or resource-constrained scenarios."
      ],
      "metadata": {
        "id": "DKKUcBtVbxuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model-Specific Special Tokens\n",
        "\n",
        "Transformer models like BERT use a small set of reserved tokens to handle sequence boundaries, unknown words, padding, and masking:\n",
        "\n",
        "| Token    | Description                                                                                 |\n",
        "|----------|---------------------------------------------------------------------------------------------|\n",
        "| `[PAD]`  | Padding token – pads shorter sequences so that all inputs in a batch have the same length (e.g. 512 tokens for BERT). |\n",
        "| `[UNK]`  | Unknown token – replaces any wordpiece/subword that isn’t in the model’s vocabulary.       |\n",
        "| `[CLS]`  | Classification token – always prepended to the input; its final hidden state is used for sequence-level tasks (e.g. classification, NSP). |\n",
        "| `[SEP]`  | Separator token – marks the end of a sentence or separates paired inputs (e.g. question vs. context). |\n",
        "| `[MASK]` | Masking token – randomly substituted for real tokens during pre-training to learn contextual representations (MLM task). |\n",
        "\n",
        "---\n",
        "\n",
        "### Example Input for BERT\n",
        "\n",
        "```text\n",
        "[CLS] What is the capital of France? [SEP] The capital of France is [MASK]. [SEP] [PAD] [PAD]\n",
        "\n",
        "\n",
        "The model sees a single sequence of fixed length (padded with [PAD]).\n",
        "\n",
        "It uses [MASK] to predict “Paris” during pre-training.\n",
        "\n",
        "During fine-tuning, the [CLS] embedding feeds into a classifier head for tasks like sentiment or NSP.\n",
        "\n",
        "Key takeaway:\n",
        "These special tokens let BERT handle variable-length inputs, denote structure (start/end), and learn from masked words—enabling powerful, unified pre-training and fine-tuning.\n"
      ],
      "metadata": {
        "id": "HQWIBxzgb4bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is a crude, rule-based way to reduce words to their “stem” or root form by stripping suffixes (and sometimes prefixes). It’s fast and language-agnostic but can be overly aggressive.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Common Algorithms\n",
        "\n",
        "| Stemmer            | Approach                                      | Example                      |\n",
        "|--------------------|-----------------------------------------------|------------------------------|\n",
        "| **PorterStemmer**  | Series of suffix-stripping rules (5 phases)  | *amazed* → **amaz**<br>*amazingly* → **amaz** |\n",
        "| **LancasterStemmer** | Iterative, conflation-based rules (more aggressive) | *amazed* → **amaz**<br>*amazingly* → **amaz** |\n",
        "| **SnowballStemmer** | Revised Porter (multi-language support)      | Similar to Porter for English |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Python Example (NLTK)\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "words = ['happy','happiest','happier','cactus','cactii',\n",
        "         'elephant','elephants','amazed','amazing','amazingly',\n",
        "         'cement','owed','maximum']\n",
        "\n",
        "porter   = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w:10} → Porter: {porter.stem(w):6} | Lancaster: {lancaster.stem(w)}\")\n",
        "\n",
        "Sample output:\n",
        "\n",
        "yaml\n",
        "Copy\n",
        "Edit\n",
        "happy      → Porter: happi  | Lancaster: happy\n",
        "happiest   → Porter: happiest | Lancaster: happiest\n",
        "happier    → Porter: happier  | Lancaster: happy\n",
        "cactus     → Porter: cactu   | Lancaster: cact\n",
        "cactii     → Porter: cactii  | Lancaster: cacti\n",
        "elephant   → Porter: elephant | Lancaster: eleph\n",
        "elephants  → Porter: elephant | Lancaster: eleph\n",
        "amazed     → Porter: amaz     | Lancaster: amaz\n",
        "amazing    → Porter: amaz     | Lancaster: amaz\n",
        "amazingly  → Porter: amazingli| Lancaster: amaz\n",
        "cement     → Porter: cement   | Lancaster: cem\n",
        "owed       → Porter: owe      | Lancaster: ow\n",
        "maximum    → Porter: maximum  | Lancaster: maxim\n",
        "3. Pros & Cons\n",
        "Pros\tCons\n",
        "▶️ Very fast and lightweight\t⚠️ Over-stemming: conflates unrelated words (e.g. “cacti”→“cact”)\n",
        "▶️ Easy to implement in any language\t⚠️ Not linguistically precise; strips affixes without context\n",
        "▶️ Reduces vocabulary size\t⚠️ Undersensitive to irregular forms (e.g. “better”→“bett”)\n",
        "\n",
        "4. When to Use\n",
        "Search & IR: quick index normalization\n",
        "\n",
        "Prototyping: feature reduction before heavier lemmatization\n",
        "\n",
        "Resource-constrained settings: when speed & memory matter\n",
        "\n",
        "Key takeaway:\n",
        "Stemming offers a fast, simple way to conflate word forms—but if you need linguistically accurate roots (e.g. “is/am/are” → “be”), consider lemmatization instead."
      ],
      "metadata": {
        "id": "Txkkmo6WdGsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization reduces words to their base or dictionary form (**lemma**) using morphological analysis and a vocabulary (e.g. WordNet), producing linguistically valid roots.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. WordNet Lemmatizer (NLTK)\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')             # ensure WordNet data is available\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words      = ['amaze', 'amazed', 'amazing']\n",
        "\n",
        "# Default (no POS tag → noun assumed)\n",
        "[lemmatizer.lemmatize(w) for w in words]\n",
        "# → ['amaze', 'amazed', 'amazing']\n",
        "\n",
        "# Specify POS = VERB\n",
        "[lemmatizer.lemmatize(w, pos=wordnet.VERB) for w in words]\n",
        "# → ['amaze', 'amaze', 'amaze']\n",
        "\n",
        "2. Pros & Cons\n",
        "Pros\tCons\n",
        "▶️ Returns valid dictionary forms (e.g. “was”→“be”)\t⚠️ Requires POS tags for best results\n",
        "▶️ Handles irregular forms and morphology\t⚠️ Slower than stemming; depends on external lexicons (WordNet)\n",
        "▶️ Improves downstream tasks by unifying variants\t⚠️ Less effective on domain-specific jargon/out-of-vocab words\n",
        "\n",
        "3. When to Use\n",
        "Information Extraction / QA: need precise base forms\n",
        "\n",
        "Text normalization: for semantic similarity or clustering\n",
        "\n",
        "Any scenario where preserving true word meaning outweighs computational cost\n",
        "\n",
        "Key takeaway:\n",
        "Lemmatization gives linguistically accurate roots by leveraging part-of-speech and lexical resources, making it superior to stemming when correctness matters."
      ],
      "metadata": {
        "id": "UNq78356exIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unicode Normalization\n",
        "\n",
        "Unicode characters can have multiple code‐point representations that look identical (or nearly so) but compare as different. Normalization transforms text into a consistent form.\n",
        "\n",
        "### 1. Canonical Equivalence (NFC / NFD)\n",
        "\n",
        "- **Canonical** means different sequences that represent the *same* abstract character(s).  \n",
        "- **Forms**  \n",
        "  - **NFC (Normalization Form C)**: composites (where possible)  \n",
        "  - **NFD (Normalization Form D)**: decomposes to base + combining marks  \n",
        "- **Examples**  \n",
        "  ```python\n",
        "  import unicodedata\n",
        "  # Ç (U+00C7) vs C + ◌̧ (U+0043 U+0327)\n",
        "  s1 = \"Ç\"\n",
        "  s2 = \"Ç\"\n",
        "  unicodedata.normalize(\"NFC\", s1) == unicodedata.normalize(\"NFC\", s2)  # True\n",
        "  unicodedata.normalize(\"NFD\", s1) == unicodedata.normalize(\"NFD\", s2)  # True\n",
        "\n",
        "\n",
        "2. Compatibility Equivalence (NFKC / NFKD)\n",
        "Compatibility means characters that look or behave similarly, but have different semantics or usage.\n",
        "\n",
        "Forms\n",
        "\n",
        "NFKC: compatibility‐decomposed, then recomposed\n",
        "\n",
        "NFKD: compatibility‐decomposed\n",
        "\n",
        "Examples\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# “①” (circled 1) vs “1”\n",
        "c1, c2 = \"①\", \"1\"\n",
        "unicodedata.normalize(\"NFKC\", c1) == unicodedata.normalize(\"NFKC\", c2)  # True\n",
        "# But NFC/NFD leave them distinct\n",
        "unicodedata.normalize(\"NFC\", c1) == unicodedata.normalize(\"NFC\", c2)    # False\n",
        "Form\tCanonical?\tCompatibility?\tUse Case\n",
        "NFC\t✅\t❌\tText display, preserving all distinctions\n",
        "NFD\t✅\t❌\tFine‐grained Unicode processing (e.g. accent analysis)\n",
        "NFKC\t❌\t✅\tSearching/comparing user‐facing text\n",
        "NFKD\t❌\t✅\tSimplifying for indexing or ASCII‐only environments\n",
        "\n",
        "3. When & Why to Normalize\n",
        "String comparison: ensure \"resumé\" == \"résume\"\n",
        "\n",
        "Search & indexing: map fullwidth or superscript digits to ASCII\n",
        "\n",
        "Data cleaning: strip out font variants, compatibility glyphs\n",
        "\n",
        "Interoperability: avoid hidden mismatches in user input, file I/O\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def normalize_text(s: str, mode: str = \"NFC\") -> str:\n",
        "    return unicodedata.normalize(mode, s)\n",
        "\n",
        "# Example\n",
        "raw = \"España\\u0301\"  # “Españá” (e + combining acute)\n",
        "print(normalize_text(raw, \"NFC\"))  # “Españá” (as single codepoint)\n",
        "Key takeaway:\n",
        "Always normalize Unicode early in your pipeline—choose NFC/NFD to preserve canonical content, or NFKC/NFKD when you need to collapse compatibility variants for reliable matching and indexing."
      ],
      "metadata": {
        "id": "W0bqHV4xgrYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unicode Normalization: Canonical & Compatibility Equivalence\n",
        "\n",
        "Different Unicode code-point sequences can look (nearly) identical but compare as unequal. Normalization transforms text into a consistent form so that equivalent sequences compare equal.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Canonical Equivalence (NFC / NFD)\n",
        "\n",
        "- **Goal:** equate characters that are *canonically* the same (same abstract character + accents), regardless of decomposition.  \n",
        "- **Forms:**  \n",
        "  - **NFD (Normalization Form D):** *canonical decomposition* → base + combining marks  \n",
        "  - **NFC (Normalization Form C):** decompose (NFD), then *recompose* where possible  \n",
        "\n",
        "| Example                              | Codepoints                                  | After NFD                      | After NFC                      |\n",
        "|--------------------------------------|----------------------------------------------|--------------------------------|--------------------------------|\n",
        "| **Ç**                                | `\\u00C7`                                     | `\\u0043` + `\\u0327`            | `\\u00C7`                        |\n",
        "| **가** (Korean “ga”)                 | `\\uAC00`                                     | `\\u1100` + `\\u1161`            | `\\uAC00`                        |\n",
        "\n",
        "```python\n",
        "import unicodedata\n",
        "\n",
        "a = \"\\u00C7\"             # single Ç\n",
        "b = \"C\\u0327\"            # C + combining cedilla\n",
        "assert a != b            # codepoints differ\n",
        "# Normalize to NFD or NFC → sequences match\n",
        "unicodedata.normalize(\"NFD\", a) == unicodedata.normalize(\"NFD\", b)   # True\n",
        "unicodedata.normalize(\"NFC\", a) == unicodedata.normalize(\"NFC\", b)   # True\n",
        "\n",
        "2. Compatibility Equivalence (NFKC / NFKD)\n",
        "Goal: also equate characters with compatibility differences (font variants, superscripts, circled forms).\n",
        "\n",
        "Forms:\n",
        "\n",
        "NFKD: compatibility decomposition (no recomposition)\n",
        "\n",
        "NFKC: compatibility decomposition + canonical recomposition\n",
        "\n",
        "Example\tLook-alike vs ASCII\tNFD / NFC\tNFKD / NFKC\n",
        "“①” (circled one)\tvs “1”\tdistinct\tboth normalize to “1” under NFKC/NFKD\n",
        "“½” (fraction one-half)\tvs “1/2”\tdistinct\tboth → “1/2” under NFKC/NFKD\n",
        "\n",
        "c1, c2 = \"①\", \"1\"\n",
        "# NFC: still distinct\n",
        "unicodedata.normalize(\"NFC\", c1) == unicodedata.normalize(\"NFC\", c2)   # False\n",
        "# NFKC: collapse compatibility variants\n",
        "unicodedata.normalize(\"NFKC\", c1) == unicodedata.normalize(\"NFKC\", c2) # True\n",
        "3. When to Use Which Form\n",
        "Form\tCanonical?\tCompatibility?\tTypical Use\n",
        "NFD\t✅\t❌\tText analysis by combining marks\n",
        "NFC\t✅\t❌\tText display & round-trip preservation\n",
        "NFKD\t❌\t✅\tIndexing / ASCII-only conversions\n",
        "NFKC\t❌\t✅\tSearch/comparison of user input\n",
        "\n",
        "4. Best Practices\n",
        "Normalize early in your pipeline (e.g. on text ingestion).\n",
        "\n",
        "Choose form based on downstream needs:\n",
        "\n",
        "NFC for preserving exact characters but canonicalizing accents.\n",
        "\n",
        "NFKC when collapsing visual or semantic variants (superscripts, circled letters).\n",
        "\n",
        "Always compare normalized strings to avoid hidden mismatches.\n",
        "\n",
        "def normalize_unicode(s: str, form: str = \"NFC\") -> str:\n",
        "    import unicodedata\n",
        "    return unicodedata.normalize(form, s)\n",
        "Key takeaway:\n",
        "Unicode normalization ensures that visually identical or semantically equivalent text compares equal—crucial for reliable matching, searching, and storage."
      ],
      "metadata": {
        "id": "U4ZzmlfXgs1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unicode Normalization: Canonical & Compatibility Equivalence\n",
        "\n",
        "Different Unicode characters can be encoded multiple ways (composed vs. decomposed, styled vs. base), so two strings that look identical may not compare equal at the code‐point level. Unicode normalization transforms text into a consistent form so that equivalent characters compare equal.\n",
        "\n",
        "### 1. Canonical Equivalence (NFC & NFD)\n",
        "\n",
        "- **Goal:** Equate characters that are *canonically* the same (same abstract letter plus accents), regardless of how they’re encoded.\n",
        "- **Forms:**\n",
        "  - **NFD (Normalization Form D)**  \n",
        "    Decomposes composed characters into base + combining marks.  \n",
        "  - **NFC (Normalization Form C)**  \n",
        "    NFD decomposition followed by recomposition into precomposed characters where possible.\n",
        "- **Example (Latin “Ç”):**  \n",
        "  - `\"\\u00C7\"` (single code point)  \n",
        "  - `\"\\u0043\\u0327\"` (“C” + combining cedilla)  \n",
        "  ```python\n",
        "  import unicodedata\n",
        "  a = \"\\u00C7\"\n",
        "  b = \"C\\u0327\"\n",
        "  assert a != b\n",
        "  # Normalize → compare equal\n",
        "  unicodedata.normalize(\"NFD\", a) == unicodedata.normalize(\"NFD\", b)   # True\n",
        "  unicodedata.normalize(\"NFC\", a) == unicodedata.normalize(\"NFC\", b)   # True\n",
        "\n",
        "2. Compatibility Equivalence (NFKC & NFKD)\n",
        "Goal: Also collapse characters that are compatibility variants (font or semantic variants, superscripts, circled forms) into their base forms.\n",
        "\n",
        "Forms:\n",
        "\n",
        "NFKD\n",
        "Compatibility decomposition (breaks font variants, superscripts, etc., into base characters and compatibility mappings).\n",
        "\n",
        "NFKC\n",
        "NFKD decomposition followed by canonical recomposition.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Circled “①” vs. “1”\n",
        "\n",
        "\n",
        "unicodedata.normalize(\"NFC\", \"①\") == \"1\"      # False\n",
        "unicodedata.normalize(\"NFKC\", \"①\") == \"1\"     # True\n",
        "Fancy H (\"\\u210B\\u0327\") vs. plain “H” (\"\\u1E28\")\n",
        "\n",
        "fancy = \"\\u210B\\u0327\"\n",
        "base  = \"\\u1E28\"\n",
        "# Before normalize: not equal\n",
        "fancy != base\n",
        "# After NFKC: both become \"H\"\n",
        "unicodedata.normalize(\"NFKC\", fancy) == base  # True\n",
        "Form\tCanonical Decomp?\tCompat. Decomp?\tRecompose?\tTypical Use\n",
        "NFD\t✅\t❌\t❌\tAccent analysis, fine‐grained text\n",
        "NFC\t✅\t❌\t✅\tText display & round‐tripping\n",
        "NFKD\t❌\t✅\t❌\tIndexing, stripping font variants\n",
        "NFKC\t❌\t✅\t✅\tSearch/comparison, ASCII folding\n",
        "\n",
        "3. Best Practices\n",
        "Normalize Early: Apply Unicode normalization at ingestion or first text‐processing step.\n",
        "\n",
        "Choose Form by Need:\n",
        "\n",
        "NFC to preserve canonical characters but unify accents.\n",
        "\n",
        "NFKC when you need to collapse compatibility variants (superscripts, circled numbers, font variants) for robust matching.\n",
        "\n",
        "Always Compare Normalized Strings: Prevent hidden mismatches in user input, searching, or data storage.\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def normalize(s: str, form: str = \"NFC\") -> str:\n",
        "    return unicodedata.normalize(form, s)\n",
        "\n",
        "# Usage:\n",
        "clean = normalize(raw_text, \"NFKC\")\n",
        "Key takeaway:\n",
        "Unicode normalization ensures that visually or semantically equivalent text compare equal—crucial for reliable matching, searching, and data integrity.\n"
      ],
      "metadata": {
        "id": "KY6fvrA9jvXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Attention"
      ],
      "metadata": {
        "id": "gy5BnOtck6Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms in Transformers (with Examples)\n",
        "\n",
        "Modern Transformer models use attention to let each token “attend” to other tokens—either within the same sequence (self‐attention) or across encoder/decoder (encoder–decoder attention). Below is a unified view with small toy examples.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Scaled Dot‐Product Attention\n",
        "\n",
        "Given:\n",
        "```python\n",
        "import numpy as np\n",
        "# Toy embeddings for two tokens (d_k=2) and two values (d_v=2):\n",
        "Q = np.array([[1.0, 0.0]])       # “hello” query\n",
        "K = np.array([[0.9, 0.1],        # “hello” key\n",
        "              [0.1, 0.9]])       # “world” key\n",
        "V = np.array([[1.0, 1.0],        # “hello” value\n",
        "              [0.0, 1.0]])       # “world” value\n",
        "\n",
        "\n",
        "Compute scores:\n",
        "\n",
        "S=QK ⊤=[[(1×0.9+0×0.1),(1×0.1+0×0.9)]]=[[0.9,0.1]]\n",
        "\n",
        "Scale by √d_k (√2≈1.41), softmax:\n",
        "\n",
        "scores = S / np.sqrt(2)           # ≈ [0.64, 0.07]\n",
        "weights = softmax(scores)        # ≈ [0.84, 0.16]\n",
        "\n",
        "Weighted sum of V:\n",
        "\n",
        "Z=weights⋅V=0.84×[1,1]+0.16×[0,1]=[0.84,1.00]\n",
        "\n",
        "Self‐Attention\n",
        "For the sequence “The cat sat”, suppose embeddings:\n",
        "\n",
        "X = [[1,0],[0,1],[1,1]]  # 3 tokens × 2 dims\n",
        "\n",
        "Build Q, K, V via linear projections (here identity for simplicity).\n",
        "\n",
        "Compute full 3×3 score matrix and softmax row‐wise:\n",
        "\n",
        "The\tcat\tsat\n",
        "The\t1⋅1+0⋅0=1 → softmax → [0.6,0.2,0.2]\n",
        "cat\t0⋅1+1⋅0=0 → [0.3,0.4,0.3]\n",
        "sat\t1⋅1+1⋅1=2 → [0.1,0.1,0.8]\n",
        "\n",
        "Outputs each mix values of The, cat, sat with those weights.\n",
        "\n",
        "Bidirectional: every token attends to both preceding and following tokens.\n",
        "\n",
        "3. Encoder–Decoder Attention\n",
        "Machine translation: Source = “hello how are you” (encoder), Target generating “ciao come va” (decoder).\n",
        "\n",
        "At decoder step t where it’s about to output “come”:\n",
        "\n",
        "  derived from decoder’s hidden “come?” state.\n",
        "\n",
        "Keys/Values from encoder outputs of each source token.\n",
        "\n",
        "Attention weights might be highest on “how” and “are” for “come.”\n",
        "\n",
        "Example weights (source tokens in order):\n",
        "\n",
        "[0.05, 0.10, 0.75, 0.10] → mostly “are”\n",
        "Then decoder context = weighted sum of encoder values.\n",
        "\n",
        "4. Multi‐Head Attention\n",
        "Say we use 2 heads, each of dimensionality 2:\n",
        "\n",
        "# Q, K, V for “hello how”\n",
        "X = np.array([[1,0,1,0],      # flattened 4-d model\n",
        "              [0,1,0,1],\n",
        "              [1,1,0,0]])\n",
        "# Head 1: take first 2 dims of Q,K,V\n",
        "# Head 2: take last 2 dims\n",
        "Each head attends differently:\n",
        "\n",
        "Head 1 might focus on “hello”→“how”\n",
        "\n",
        "Head 2 might focus on “how”→“you”\n",
        "\n",
        "Their outputs (3×2 + 3×2) concatenate into 3×4, then project back to 3×4.\n",
        "\n",
        "5. Layer Integration\n",
        "In each Transformer layer (encoder or decoder):\n",
        "\n",
        "Multi‐Head Self‐Attention → Add & Norm\n",
        "\n",
        "(Decoder only) Encoder–Decoder Multi‐Head Attention → Add & Norm\n",
        "\n",
        "Positionwise Feed‐Forward → Add & Norm\n",
        "\n",
        "Stacking\n",
        "𝐿\n",
        "L such layers builds deep contextual encoders/decoders.\n",
        "\n",
        "Key Takeaway:\n",
        "Attention computes weighted sums of values using similarity of queries to keys. Self‐attention lets tokens interact bidirectionally, encoder–decoder attention connects source to target, and multi‐head expands representational capacity—all enabling powerful sequence modeling."
      ],
      "metadata": {
        "id": "oYTSG6oJk9yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Clasiffication"
      ],
      "metadata": {
        "id": "UiGn_9Z7sRkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Mechanisms in Transformers (with Examples)\n",
        "\n",
        "Modern Transformer models use attention to let each token “attend” to other tokens—either within the same sequence (self‐attention) or across encoder/decoder (encoder–decoder attention). Below is a unified view with small toy examples.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Scaled Dot‐Product Attention\n",
        "\n",
        "Given:\n",
        "```python\n",
        "import numpy as np\n",
        "# Toy embeddings for two tokens (d_k=2) and two values (d_v=2):\n",
        "Q = np.array([[1.0, 0.0]])       # “hello” query\n",
        "K = np.array([[0.9, 0.1],        # “hello” key\n",
        "              [0.1, 0.9]])       # “world” key\n",
        "V = np.array([[1.0, 1.0],        # “hello” value\n",
        "              [0.0, 1.0]])       # “world” value\n",
        "\n",
        "Compute scores:\n",
        "S=QK ⊤ =[[(1×0.9+0×0.1),(1×0.1+0×0.9)]]=[[0.9,0.1]]\n",
        "\n",
        "Scale by √d_k (√2≈1.41), softmax:\n",
        "scores = S / np.sqrt(2)           # ≈ [0.64, 0.07]\n",
        "weights = softmax(scores)        # ≈ [0.84, 0.16]\n",
        "\n",
        "Weighted sum of V:\n",
        "Z=weights⋅V=0.84×[1,1]+0.16×[0,1]=[0.84,1.00]\n",
        "\n",
        "\n",
        "Self‐Attention\n",
        "For the sequence “The cat sat”, suppose embeddings:\n",
        "\n",
        "X = [[1,0],[0,1],[1,1]]  # 3 tokens × 2 dims\n",
        "\n",
        "Build Q, K, V via linear projections (here identity for simplicity).\n",
        "\n",
        "Compute full 3×3 score matrix and softmax row‐wise:\n",
        "\n",
        "The\tcat\tsat\n",
        "The\t1⋅1+0⋅0=1 → softmax → [0.6,0.2,0.2]\n",
        "cat\t0⋅1+1⋅0=0 → [0.3,0.4,0.3]\n",
        "sat\t1⋅1+1⋅1=2 → [0.1,0.1,0.8]\n",
        "\n",
        "Outputs each mix values of The, cat, sat with those weights.\n",
        "\n",
        "Bidirectional: every token attends to both preceding and following tokens.\n",
        "\n",
        "3. Encoder–Decoder Attention\n",
        "Machine translation: Source = “hello how are you” (encoder), Target generating “ciao come va” (decoder).\n",
        "\n",
        "At decoder step t where it’s about to output “come”:\n",
        "  derived from decoder’s hidden “come?” state.\n",
        "\n",
        "Keys/Values from encoder outputs of each source token.\n",
        "\n",
        "Attention weights might be highest on “how” and “are” for “come.”\n",
        "\n",
        "Example weights (source tokens in order):\n",
        "\n",
        "\n",
        "[0.05, 0.10, 0.75, 0.10] → mostly “are”\n",
        "Then decoder context = weighted sum of encoder values.\n",
        "\n",
        "4. Multi‐Head Attention\n",
        "Say we use 2 heads, each of dimensionality 2:\n",
        "\n",
        "# Q, K, V for “hello how”\n",
        "X = np.array([[1,0,1,0],      # flattened 4-d model\n",
        "              [0,1,0,1],\n",
        "              [1,1,0,0]])\n",
        "# Head 1: take first 2 dims of Q,K,V\n",
        "# Head 2: take last 2 dims\n",
        "Each head attends differently:\n",
        "\n",
        "Head 1 might focus on “hello”→“how”\n",
        "\n",
        "Head 2 might focus on “how”→“you”\n",
        "\n",
        "Their outputs (3×2 + 3×2) concatenate into 3×4, then project back to 3×4.\n",
        "\n",
        "5. Layer Integration\n",
        "In each Transformer layer (encoder or decoder):\n",
        "\n",
        "Multi‐Head Self‐Attention → Add & Norm\n",
        "\n",
        "(Decoder only) Encoder–Decoder Multi‐Head Attention → Add & Norm\n",
        "\n",
        "Positionwise Feed‐Forward → Add & Norm\n",
        "\n",
        "Stacking\n",
        "𝐿\n",
        "L such layers builds deep contextual encoders/decoders.\n",
        "\n",
        "Key Takeaway:\n",
        "Attention computes weighted sums of values using similarity of queries to keys. Self‐attention lets tokens interact bidirectionally, encoder–decoder attention connects source to target, and multi‐head expands representational capacity—all enabling powerful sequence modeling.\n",
        "\n"
      ],
      "metadata": {
        "id": "Es-HDrgBnx1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Attention: Concepts & Concrete Examples\n",
        "\n",
        "Transformers power tasks from translation to sentiment analysis by using **attention** to let each token selectively “listen” to other tokens. Below is a unified overview **with small, worked examples** at each step.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Scaled Dot-Product Attention\n",
        "\n",
        "### Setup\n",
        "\n",
        "Suppose we have 2 tokens (“hello”, “world”), each with a 2-dimensional key/query/value embedding:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Query: “hello”\n",
        "Q = np.array([[1.0, 0.0]])        # shape (1×2)\n",
        "\n",
        "# Keys:\n",
        "K = np.array([\n",
        "    [0.9, 0.1],  # “hello” key\n",
        "    [0.1, 0.9],  # “world” key\n",
        "])  # shape (2×2)\n",
        "\n",
        "# Values:\n",
        "V = np.array([\n",
        "    [1.0, 1.0],  # “hello” value\n",
        "    [0.0, 1.0],  # “world” value\n",
        "])  # shape (2×2)\n",
        "\n",
        "1.1 Compute raw scores\n",
        "\n",
        "S=QK ⊤ =[1×0.9+0×0.1,1×0.1+0×0.9]=[0.9,0.1]\n",
        "\n",
        "1.2 Scale & Softmax\n",
        "Divide by ≈≈1.414 to stabilize gradients, then softmax:\n",
        "\n",
        "scores = S / np.sqrt(2)            # ≈ [0.636, 0.071]\n",
        "weights = np.exp(scores) / np.sum(np.exp(scores))\n",
        "# ≈ [0.84, 0.16]\n",
        "\n",
        "1.3 Weighted sum → output\n",
        "Z=weights⋅V=0.84[1,1]+0.16[0,1]=[0.84,1.00]\n",
        "\n",
        "2. Self-Attention (Within One Sequence)\n",
        "For a 3-word sentence “A B C” with toy embeddings:\n",
        "X = np.array([\n",
        "  [1, 0],   # “A”\n",
        "  [0, 1],   # “B”\n",
        "  [1, 1],   # “C”\n",
        "])  # shape (3×2)\n",
        "Q=K=V = X  # identity projections here for simplicity\n",
        "\n",
        "Compute all pairwise dot-products → 3×3 matrix S.\n",
        "\n",
        "Scale, row-wise softmax → each row sums to 1.\n",
        "\n",
        "Each output row is a weighted sum of V.\n",
        "E.g. row for “C” might attend mostly to itself:\n",
        "\n",
        "Compute all pairwise dot-products → 3×3 matrix S.\n",
        "\n",
        "Scale, row-wise softmax → each row sums to 1.\n",
        "\n",
        "Each output row is a weighted sum of V.\n",
        "E.g. row for “C” might attend mostly to itself:\n",
        "softmax([1+0,0+1,1+1])=softmax([1,1,2])≈[0.21,0.21,0.58].\n",
        "\n",
        "Bidirectional: every token (A, B, C) can look left and right.\n",
        "\n",
        "3. Encoder–Decoder (Cross) Attention\n",
        "In translation, the encoder processes source “how are you” and produces hidden states {h1, h2, h3} .\n",
        ". At decoder step for target token “come”:\n",
        "Decoder hidden → query qt\n",
        "Encoder states → keys {k1} and values {v1}\n",
        "Dot-product + softmax gives weights over source:\n",
        "[0.05, 0.15, 0.80]  # mostly attends to “you”\n",
        "Context = weighted sum of values → informs decoding of “come.”\n",
        "\n",
        " Multi-Head Attention\n",
        "Instead of one attention, we use H parallel heads, each on its own linear projection of Q/K/V:\n",
        "\n",
        "   Input X (seq_len×d_model)\n",
        "     ↓\n",
        "  ┌───────────────────────────┐\n",
        "  │ Head₁: Linear→Attention→│\n",
        "  │ Head₂: Linear→Attention→│  → Concatenate → Linear → Output\n",
        "  │ …                         │\n",
        "  │ Head_H                    │\n",
        "  └───────────────────────────┘\n",
        "Example: d_model=4, H=2, so each head has d_k=d_v=2.\n",
        "\n",
        "Head 1 might focus on local word co-occurrence; Head 2 on longer-range patterns.\n",
        "\n",
        "Their two (seq×2) outputs are concatenated → (seq×4) → final projection.\n",
        "\n",
        "5. Putting It All Together\n",
        "A single Transformer layer (encoder side) looks like:\n",
        "\n",
        "Multi-Head Self-Attention → Add & Norm\n",
        "\n",
        "Position-wise Feed-Forward → Add & Norm\n",
        "\n",
        "A decoder layer adds:\n",
        "\n",
        "Masked multi-head self-attention\n",
        "\n",
        "Encoder–decoder multi-head attention\n",
        "\n",
        "Feed-forward\n",
        "\n",
        "Stacking\n",
        "𝐿\n",
        "L layers yields deep models (e.g., BERT has 12–24 encoder layers).\n",
        "\n",
        "Why It Works\n",
        "Vector similarity (dot-product) lets tokens dynamically share information.\n",
        "\n",
        "Multiple heads learn different “eyes” over the sequence.\n",
        "\n",
        "Layer stacking builds rich contextual representations used for classification, generation, and more.\n",
        "\n",
        "Quick Recap\n",
        "Attention Type\tQuery from…\tKey/Value from…\tUse Case\n",
        "Self-Attention\tsame layer inputs\tsame layer inputs\tContextual encoding (BERT)\n",
        "Cross-Attention\tdecoder\tencoder outputs\tSeq2seq (translation, etc.)\n",
        "Multi-Head\tparallel selves\tparallel selves\tExpand representational power\n",
        "\n",
        "Each component is fully differentiable—so models learn what to attend to, from raw data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mrFMAguNsYSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Classification in Python\n",
        "\n",
        "Below are two complete examples—one using **Flair** and one using Hugging Face’s **Transformers**—all in one single Markdown snippet. Simply copy & paste into your own notes or notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Flair\n",
        "\n",
        "```bash\n",
        "# Install Flair\n",
        "pip install flair\n",
        "\n",
        "# 1️⃣ Initialize the model\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "\n",
        "model = flair.models.TextClassifier.load('en-sentiment')\n",
        "\n",
        "# 2️⃣ Tokenize + wrap text\n",
        "text = \"I like you!\"\n",
        "sentence = Sentence(text)\n",
        "\n",
        "# 3️⃣ Predict sentiment\n",
        "model.predict(sentence)\n",
        "\n",
        "# 4️⃣ Inspect the result\n",
        "print(sentence)\n",
        "# → Sentence: \"I like you !\"  [– Tokens: 4 – Sentence-Labels: {'label': [POSITIVE (0.9928)]}]\n",
        "\n",
        "label = sentence.get_labels()[0]\n",
        "print(label.value, label.score)\n",
        "# → POSITIVE 0.9928\n",
        "\n",
        "# 5️⃣ Another example (negative)\n",
        "text = \"I hate it when I'm not learning about ML\"\n",
        "sentence = Sentence(text)\n",
        "model.predict(sentence)\n",
        "neg_label = sentence.get_labels()[0]\n",
        "print(neg_label.value, neg_label.score)\n",
        "# → NEGATIVE 0.9991\n",
        "\n",
        "2. Hugging Face Transformers\n",
        "\n",
        "# Install Transformers and PyTorch\n",
        "pip install transformers torch\n",
        "\n",
        "# 1️⃣ Load model & tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "model_name = \"ProsusAI/finbert\"\n",
        "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
        "model      = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 2️⃣ Build a sentiment-analysis pipeline\n",
        "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# 3️⃣ Run on a single sentence\n",
        "result = nlp(\"I love machine learning!\")\n",
        "print(result)\n",
        "# → [{'label': 'POSITIVE', 'score': 0.9993}]\n",
        "\n",
        "# 4️⃣ Batch inference\n",
        "texts = [\n",
        "    \"That movie was fantastic!\",\n",
        "    \"The service was terrible and slow.\"\n",
        "]\n",
        "results = nlp(texts)\n",
        "for txt, res in zip(texts, results):\n",
        "    print(f\"{txt}  →  {res['label']} ({res['score']:.2f})\")\n",
        "\n",
        "Appendix: Special Token IDs (BERT-style)\n",
        "\n",
        "| Token    |  ID |\n",
        "| :------- | :-: |\n",
        "| `[CLS]`  | 101 |\n",
        "| `[SEP]`  | 102 |\n",
        "| `[MASK]` | 103 |\n",
        "| `[UNK]`  | 100 |\n",
        "| `[PAD]`  |  0  |\n",
        "\n",
        "## 3. Raw Model Outputs & Post‐processing\n",
        "\n",
        "Sometimes you want to call the model directly (without the pipeline helper) and then convert its raw logits into probabilities and labels yourself:\n",
        "\n",
        "```python\n",
        "# Assume you've already done:\n",
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model     = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# 1️⃣ Tokenize + get kwargs dict\n",
        "tokens = tokenizer.encode_plus(\n",
        "    txt,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# 2️⃣ Forward pass: unpack **tokens into model()\n",
        "output = model(**tokens)\n",
        "# output is a SequenceClassifierOutput with `logits`\n",
        "\n",
        "# 3️⃣ Extract the first (and only) batch element’s logits\n",
        "logits = output[0]            # → tensor([-1.8200,  2.4484,  0.0216])\n",
        "\n",
        "# 4️⃣ Convert logits to probabilities with softmax\n",
        "import torch.nn.functional as F\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "print(probs)\n",
        "# → tensor([0.0127, 0.9072, 0.0801])\n",
        "\n",
        "# 5️⃣ Pick the index of the highest‐probability class\n",
        "import torch\n",
        "predicted_class_idx = torch.argmax(probs).item()\n",
        "print(predicted_class_idx)\n",
        "# → 1\n",
        "\n",
        "# 6️⃣ Map index → label (you can inspect `model.config.id2label`)\n",
        "label = model.config.id2label[predicted_class_idx]\n",
        "print(label, probs[predicted_class_idx].item())\n",
        "# → “POSITIVE” 0.9072\n",
        "\n",
        "\n",
        "Summary\n",
        "Tokenize → a dict of input_ids, attention_mask, (…)\n",
        "\n",
        "Unpack dict into model(**tokens) → raw logits\n",
        "\n",
        "Softmax → probabilities\n",
        "\n",
        "Argmax → predicted class index\n",
        "\n",
        "Lookup index in model.config.id2label for final label"
      ],
      "metadata": {
        "id": "sn00RWBYrUeL"
      }
    }
  ]
}